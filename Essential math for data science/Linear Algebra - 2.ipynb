{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f05b0efc",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Linear Algebra foundation for Data Science\"\n",
    "format: \n",
    "    revealjs:\n",
    "        slide-number: true\n",
    "        theme: dark\n",
    "        width: 1600\n",
    "        height: 900\n",
    "        mainfont: Lato\n",
    "        highlight-style: vim-dark\n",
    "        incremental: true\n",
    "        footer: \"Manish Patel\"\n",
    "        progress: true\n",
    "jupyter: python3\n",
    "embed-resources: true\n",
    "execute:\n",
    "  echo: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a64481",
   "metadata": {},
   "source": [
    "## Linear dependence and independence "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0b951",
   "metadata": {},
   "source": [
    "- A set of vectors is **linearly dependent** if at least one vector can be obtained as a linear combination of other vectors in the set. As you can see in the left pane, we can combine vectors $x$ and $y$ to obtain $z$. \n",
    "\n",
    "- The important points to remember are: `linearly dependent vectors contain redundant information, whereas linearly independent vectors do not.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4ea426",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59028c85",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./images/b-linear-independence.svg\">\n",
    "<center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0931f5e1",
   "metadata": {},
   "source": [
    "## Linear Dependence {background-iframe=\"https://www.geogebra.org/m/qbp6ymgw\" background-interactive=true}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467d18eb",
   "metadata": {},
   "source": [
    "## Vector null space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ba0e5",
   "metadata": {},
   "source": [
    "Intuitively, the null space of a set of vectors are **all linear combinations that \"map\" into the zero vector**.   \n",
    "Consider a set of geometric vectors $\\bf{w}$, $\\bf{x}$, $\\bf{y}$, and $\\bf{z}$ as in **Fig. 8**.   \n",
    "By inspection, we can see that vectors $\\bf{x}$ and $\\bf{z}$ are parallel to each other, hence, independent.   \n",
    "On the contrary, vectors $\\bf{w}$ and $\\bf{y}$ can be obtained as linear combinations of $\\bf{x}$ and $\\bf{z}$, therefore, dependent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b62034",
   "metadata": {},
   "source": [
    "## {.smaller} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f501fd2b",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./images/b-vector-null-space.svg\">\n",
    "<center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae399d",
   "metadata": {},
   "source": [
    "## Vector norms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a082bc8d",
   "metadata": {},
   "source": [
    "- The **norm** or the **length** of a vector as the distance between its \"origin\" and its \"end\".  \n",
    "\n",
    "- Norms \"map\" vectors to non-negative values. In this sense are functions that assign length $\\lVert \\bf{x} \\rVert \\in \\mathbb{R^n}$ to a vector $\\bf{x}$. To be valid, a norm has to satisfy these properties (keep in mind these properties are a bit abstruse to understand):\n",
    "\n",
    "1. **`Absolutely homogeneous`**: $\\forall \\alpha \\in \\mathbb{R},  \\lVert \\alpha \\bf{x} \\rVert = \\vert \\alpha \\Vert \\lVert \\bf{x} \\rVert$. In words: for all real-valued scalars, the norm scales proportionally with the value of the scalar.\n",
    "2. **`Triangle inequality`**:  $\\lVert \\bf{x} + \\bf{y} \\rVert \\le \\lVert \\bf{x} \\rVert + \\lVert \\bf{y} \\rVert$. In words: in geometric terms, for any triangle the sum of any two sides must be greater or equal to the lenght of the third side. This is easy to see experimentally: grab a piece of rope, form triangles of different sizes, measure all the sides, and test this property.\n",
    "3. **`Positive definite`**: $\\lVert \\bf{x} \\rVert \\ge 0$ and $\\lVert \\bf{x} \\rVert = 0 \\Longleftrightarrow \\bf{x}= 0$. In words: the length of any $\\bf{x}$ has to be a positive value (i.e., a vector can't have negative length), and a length of $0$ occurs only of $\\bf{x}=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cf9057",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313142ac",
   "metadata": {},
   "source": [
    "<center> Fig. 9: Vector norms <center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7d48ab",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./images/b-l2-norm.svg\">\n",
    "<center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8e0ceb",
   "metadata": {},
   "source": [
    "## Euclidean norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc734770",
   "metadata": {},
   "source": [
    "The Euclidean norm is one of the most popular norms in machine learning. It is so widely used that sometimes is refered simply as \"the norm\" of a vector. Is defined as:\n",
    "\n",
    "$$\n",
    "\\lVert \\bf{x} \\rVert_2 := \\sqrt{\\sum_{i=1}^n x_i^2} = \\sqrt{x^Tx} \n",
    "$$\n",
    "\n",
    "Hence, in **two dimensions** the $L_2$ norm is:\n",
    "\n",
    "$$\n",
    "\\lVert \\bf{x} \\rVert_2 \\in \\mathbb{R}^2 = \\sqrt {x_1^2  \\cdot x_2^2 } \n",
    "$$\n",
    "\n",
    "Which is equivalent to the formula for the hypotenuse a triangle with sides $x_1^2$ and $x_2^2$. \n",
    "\n",
    "The same pattern follows for higher dimensions of $\\mathbb{R^n}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ea6c13",
   "metadata": {},
   "source": [
    "## Slide Title {background-iframe=\"https://www.geogebra.org/m/Scz8gTWv\" background-interactive=true}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83d8094",
   "metadata": {},
   "source": [
    "## Numpy norm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98eff1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[3],[4]])\n",
    "\n",
    "np.linalg.norm(x, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5faca9",
   "metadata": {},
   "source": [
    "## Manhattan norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9cbec1",
   "metadata": {},
   "source": [
    "The Manhattan or $L_1$ norm gets its name in analogy to measuring distances while moving in Manhattan, NYC. Since Manhattan has a grid-shape, the distance between any two points is measured by moving in vertical and horizontals lines (instead of diagonals as in the Euclidean norm). It is defined as:\n",
    "\n",
    "$$\n",
    "\\lVert \\bf{x} \\rVert_1 := \\sum_{i=1}^n \\vert x_i \\vert \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4083a4",
   "metadata": {},
   "source": [
    "Where $\\vert x_i \\vert$ is the absolute value. The $L_1$ norm is preferred when discriminating between elements that are exactly zero and elements that are small but not zero.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83af0fa",
   "metadata": {},
   "source": [
    "## Numpy Manhattan norm \n",
    "\n",
    "::: {.nonincremental}\n",
    "\n",
    "\n",
    "- In `NumPy` we compute the $L_1$ norm as\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74ebce58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[3],[-4]])\n",
    "\n",
    "np.linalg.norm(x, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4f3afd",
   "metadata": {},
   "source": [
    "## Slide Title {background-iframe=\"https://www.geogebra.org/m/vsj8akcb\" background-interactive=true}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd04842",
   "metadata": {},
   "source": [
    "## Max norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60a9716",
   "metadata": {},
   "source": [
    "The max norm or infinity norm is simply the absolute value of the largest element in the vector. It is defined as:\n",
    "\n",
    "$$\n",
    "\\lVert \\bf{x} \\rVert_\\infty := max_i \\vert x_i \\vert \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc2dc50",
   "metadata": {},
   "source": [
    "Where $\\vert x_i \\vert$  is the absolute value. For instance, for a vector with elements $\\bf{x} = \\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix}$, the $\\lVert \\bf{x} \\rVert_\\infty = 3$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3718395e",
   "metadata": {},
   "source": [
    "## Numpy Max norm\n",
    "\n",
    "::: {.nonincremental}\n",
    "\n",
    " - In `NumPy` we compute the $L_\\infty$ norm as:\n",
    " \n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3736a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[3],[-4]])\n",
    "\n",
    "np.linalg.norm(x, np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c56f112",
   "metadata": {},
   "source": [
    "# Vector inner product, length, and distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b7fd1",
   "metadata": {},
   "source": [
    "## Inner Product\n",
    "The notation for the inner product is usually a pair of angle brackets as $\\langle  .,. \\rangle$ .   \n",
    "For instance, the scalar inner product is defined as:\n",
    "\n",
    "$$\n",
    "\\langle x,y \\rangle := x\\cdot y\n",
    "$$\n",
    "\n",
    "In $\\mathbb{R}^n$ the inner product is a dot product defined as:\n",
    "\n",
    "$$\n",
    "\\Bigg \\langle\n",
    "\\begin{bmatrix} \n",
    "x_1 \\\\ \n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix} \n",
    "y_1 \\\\ \n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{bmatrix}\n",
    "\\Bigg \\rangle :=\n",
    "x \\cdot y = \\sum_{i=1}^n x_iy_i\n",
    "$$\n",
    "\n",
    "## Length\n",
    "\n",
    "**Length** is a concept from geometry.     \n",
    "We say that geometric vectors have length and that vectors in $\\mathbb{R}^n$ have norm. \n",
    "For instance, we can compute the length of a directed segment (i.e., geometrical vector) $\\bf{x}$ by taking the square root of the inner product with itself as:\n",
    "\n",
    "$$\n",
    "\\lVert x \\rVert = \\sqrt{\\langle x,x \\rangle} = \\sqrt{x\\cdot y} = x^2 + y^2  \n",
    "$$\n",
    "\n",
    "## Distance\n",
    "**Distance** is a relational concept. It refers to the length (or norm) of the difference between two vectors. Hence, we use norms and lengths to measure the distance between vectors. Consider the vectors $\\bf{x}$ and $\\bf{y}$, we define the distance $d(x,y)$ as:\n",
    "\n",
    "$$\n",
    "d(x,y) := \\lVert x - y \\rVert = \\sqrt{\\langle x - y, x - y \\rangle}\n",
    "$$\n",
    "\n",
    "When the inner product $\\langle x - y, x - y \\rangle$ is the dot product, the distance equals to the Euclidean distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53173c50",
   "metadata": {},
   "source": [
    "## Numpy dot product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffeab1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-14]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = np.array([[-2],[2]]), np.array([[4],[-3]])\n",
    "x.T @ y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e28376",
   "metadata": {},
   "source": [
    "## Numpy Distance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0310ff03",
   "metadata": {},
   "source": [
    "As with the inner product, usually, we can safely assume that **distance** stands for the Euclidean distance or $L_2$ norm unless otherwise noted. To compute the $L_2$ distance between a pair of vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4fd6eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_2 distance : 7.810249675906656\n"
     ]
    }
   ],
   "source": [
    "distance = np.linalg.norm(x-y, 2)\n",
    "print(f'L_2 distance : {distance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c607aaca",
   "metadata": {},
   "source": [
    "## Vector angles and orthogonality {.smaller}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b6fae8",
   "metadata": {},
   "source": [
    "In machine learning, the **angle** between a pair of vectors is used as a **measure of vector similarity**.   \n",
    "\n",
    "To understand angles let's first look at the **Cauchy–Schwarz inequality**. Consider a pair of non-zero vectors $\\bf{x}$ and $\\bf{y}$ $\\in \\mathbb{R}^n$. The Cauchy–Schwarz inequality states that:  \n",
    "\n",
    "$$\n",
    "\\vert \\langle x, y \\rangle \\vert \\leq \\Vert x \\Vert \\Vert y \\Vert\n",
    "$$\n",
    "\n",
    "In words: *the absolute value of the inner product of a pair of vectors is less than or equal to the products of their length*. The only case where both sides of the expression are *equal* is when vectors are colinear, for instance, when $\\bf{x}$ is a scaled version of $\\bf{y}$. In the 2-dimensional case, such vectors would lie along the same line. \n",
    "\n",
    "The definition of the angle between vectors can be thought as a generalization of the **law of cosines** in trigonometry, which defines for a triangle with sides $a$, $b$, and $c$, and an angle $\\theta$ are related as:\n",
    "\n",
    "$$\n",
    "c^2 = a^2 + b^2 - 2ab \\cos \\theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824870ab",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb55cd9d",
   "metadata": {},
   "source": [
    "<center> Fig. 10: Law of cosines and Angle between vectors <center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa23bbb9",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./images/b-vector-angle.svg\">\n",
    "<center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67cc8fd",
   "metadata": {},
   "source": [
    "## COSINE ANGLE {.smaller}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd26cbd",
   "metadata": {},
   "source": [
    "We can replace this expression with vectors lengths as: \n",
    "\n",
    "$$\n",
    "\\Vert x - y \\Vert^2 = \\Vert x \\Vert^2 + \\Vert y \\Vert^2 - 2(\\Vert x \\Vert \\Vert y \\Vert) \\cos \\theta\n",
    "$$\n",
    "\n",
    "With a bit of algebraic manipulation, we can clear the previous equation to:\n",
    "\n",
    "$$\n",
    "\\cos \\theta = \\frac{\\langle x, y \\rangle}{\\Vert x \\Vert \\Vert y \\Vert} \n",
    "$$\n",
    "\n",
    "And there we have a **definition for (cos) angle $\\theta$**. Further, from the Cauchy–Schwarz inequality we know that $\\cos \\theta$ must be:\n",
    "\n",
    "$$\n",
    "-1 \\leq \\frac{\\langle x, y \\rangle}{\\Vert x \\Vert \\Vert y \\Vert} \\leq 1  \n",
    "$$\n",
    "\n",
    "This is a necessary conclusion (range between $\\{-1, 1\\}$) since the numerator in the equation always is going to be smaller or equal to the denominator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6993ba",
   "metadata": {},
   "source": [
    "## NUMPY COSINE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae8f7b5",
   "metadata": {},
   "source": [
    "In `NumPy`, we can compute the $\\cos \\theta$ between a pair of vectors as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51973db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos of the angle = [[0.988]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x, y = np.array([[1], [2]]), np.array([[5], [7]])\n",
    "\n",
    "# here we translate the cos(theta) definition\n",
    "cos_theta = (x.T @ y) / (np.linalg.norm(x,2) * np.linalg.norm(y,2))\n",
    "print(f'cos of the angle = {np.round(cos_theta, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d168472c",
   "metadata": {},
   "source": [
    "We get that $\\cos \\theta \\approx 0.988$. Finally, to know the exact value of $\\theta$ we need to take the trigonometric inverse of the cosine function as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33732d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angle in radians = [[0.157]]\n"
     ]
    }
   ],
   "source": [
    "cos_inverse = np.arccos(cos_theta)\n",
    "print(f'angle in radians = {np.round(cos_inverse, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390f4c85",
   "metadata": {},
   "source": [
    "We obtain $\\theta \\approx 0.157 $. To fo from radiants to degrees we can use the following formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4308fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angle in degrees = [[8.973]]\n"
     ]
    }
   ],
   "source": [
    "degrees = cos_inverse * ((180)/np.pi)\n",
    "print(f'angle in degrees = {np.round(degrees, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ee501f",
   "metadata": {},
   "source": [
    "We obtain $\\theta \\approx 8.973^{\\circ}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1400b7",
   "metadata": {},
   "source": [
    "## Orthogonality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd328c2",
   "metadata": {},
   "source": [
    "- `Orthogonality` can be seen as a generalization of perpendicularity to vectors in any number of dimensions.\n",
    "\n",
    "- We say that a pair of vectors $\\bf{x}$ and $\\bf{y}$ are **orthogonal** if their inner product is zero, $\\langle x,y \\rangle = 0$. The notation for a pair of orthogonal vectors is $\\bf{x} \\perp \\bf{y}$. In the 2-dimensional plane, this equals to a pair of vectors forming a $90^{\\circ}$ angle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c980e6ed",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc09005",
   "metadata": {},
   "source": [
    "<center> Fig. 11: Orthogonal vectors <center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7497ebf8",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./images/b-orthogonal-vectors.svg\">\n",
    "<center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9dcc3f",
   "metadata": {},
   "source": [
    "## Numpy Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "052f78de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos of the angle = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[2], [0]])\n",
    "y = np.array([[0], [2]])\n",
    "\n",
    "cos_theta = (x.T @ y) / (np.linalg.norm(x,2) * np.linalg.norm(y,2))\n",
    "print(f'cos of the angle = {np.round(cos_theta, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276984a1",
   "metadata": {},
   "source": [
    "We see that this vectors are **orthogonal** as $\\cos \\theta=0$. This is equal to  $\\approx 1.57$ radians and $\\theta = 90^{\\circ}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c0cd2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angle in radiants = [[1.571]]\n",
      "angle in degrees =[[90.]] \n"
     ]
    }
   ],
   "source": [
    "cos_inverse = np.arccos(cos_theta)\n",
    "degrees = cos_inverse * ((180)/np.pi)\n",
    "print(f'angle in radiants = {np.round(cos_inverse, 3)}\\nangle in degrees ={np.round(degrees, 3)} ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
